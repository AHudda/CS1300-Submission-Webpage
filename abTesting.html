<!DOCTYPE html>
  <html lang="en">
    <head>
      <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <title>A/B Testing Submission Webpage</title>
      <!-- import CSS styles -->
      <link rel="stylesheet" href="abTestingStyles.css" /> 
    </head>

    <body>
      <div id="pageHeader">
        <h1 class="pageHeaderText">A/B Testing</h1>
        <h2 class="pageSubheaderText">March 14th, 2024</h2>
      </div>
      <!-- Introduction -->
      <div class="sectionSpecifications">
        <h3 class="headerText">I. Introduction</h3>
        <p class="bodyText">My objective for this project was to gain a better understanding of A/B testing methodologies and statistical analysis techniques 
          (t-tests & chi-squared tests). By gathering data for two UI designs for a webpage, conducting statistical tests on user interaction metrics, and 
          interpreting the results, I learned how to form appropriate conclusions regarding how design modifications in Version B impacted user interaction. 
          Specifically, I evaluated three user interaction metrics: misclick rate, time spent on page, time until the user first executed a click.</p>
        <p class="bodyText">Here are the overarching questions for this project: how do the design modifications made in Version B impact user interaction, 
          compared to Version A? Does Version B improve user interactions metrics, in comparison to Version A? A/B testing is particularly helpful in evaluating 
          these questions because it allows for a systematic comparison between user interaction data collected from Versions A and B of the webpage. Using 
          statistical tests to interpret the raw data, A/B testing provides insights into different user interaction metrics, which can then be compared/contrasted 
          between versions to refine design based on empirical evidence.</p>
      </div>
      
      <!-- Data Collection -->
      <div class="sectionSpecifications">
        <h3 class="headerText">II. Data Collection</h3>
        <h4 class="subHeaderText">Methodology</h4>
        <p class="bodyText">Data for this project was collected during a studio session of approximately 40 students total. Students took turns completing a specific 
          task on a TA's computer to gather data for Version A. Next, students made changes to Version A of the webpage to create Version B. Lastly, students walked
          around and completed the same specific task on each other's computers to gather data for Version B. The specific task was to select an appointment with a 
          specific person, and at a speciifc location, date and time.
        </p>
        <!-- Website Images -->
        <div class="websiteVersions">
          <div>
            <h4 class="subHeaderText">Version A</h4>
            <img class="imageSpecifications" alt="version A of webpage" src="./assets/ab_testing/versionA.png"/>
          </div>
          <div>
            <h4 class="subHeaderText">Version B</h4>
            <img class="imageSpecifications" alt="version B of webpage" src="./assets/ab_testing/versionB.png"/>
          </div>
        </div>
        <!-- HTML Images -->
        <p class="bodyText">The following changes were made to Version A to create Version B:</p>
        <ul>
          <li>Deleted the 'See Appointment' Button</li>
          <li>Changed color of the 'Select Appointment' Button to increase color contrast between button/text</li>
          <li>When the user hovers over the 'Select Appointment' Button it's outlined in black</li>
          <li>Added dividers to indicate which button corresponds with which appointment</li>
          <li>Organized appointments in chronological order based on the date they occur on</li>
        </ul>
      </div>
      
      <!-- Analysis-->
      <div class="sectionSpecifications">
        <h3 class="headerText">III. Analysis</h3>
        <!-- Misclick Rate -->
        <h4 class="subHeaderText">i. Misclick Rate</h4>
        <p class="bodyText additionalIndent">Hypotheses</p>
          <ul class="additionalIndent">
            <li><span class="boldText">Metric Of Choice:</span> The metric of choice is did_misclick, which is a boolean flag indicating if the user pushed a 
              button external to the task.</li>
            <li><span class="boldText">Null Hypothesis: </span> The distribution of misclicks is the same for Version A and Version B.</li>
              <ul>
                <li>I predict that I will reject the null hypothesis. As noted in my justification for my alternative hypothesis, design changes introduced in 
                  Version B include fewer buttons, clearer color contrast for the buttons, button hover effects, adding dividers, and ordering the appointments 
                  chronologically. These design modifications were strategically made to improve the readability and structure of the webpage. I predict the design 
                  differences between Versions A and B will influence user interaction with the page. After skimming the data, it appears that more people 
                  misclicked in Version A. Due to the design differences between Versions A and B and the observed discrepancy between the two versions’ misclicks, 
                  I predict that the statistical test’s results will find statistically significant differences in the distributions of misclicks for Versions A 
                  and B. Thus, I predict that I will find statistically significant differences and reject the null hypothesis.
                </li>
              </ul>
            <li><span class="boldText">Alternative Hypothesis:</span> The distribution of misclicks is different for Version A and Version B.</li>
              <ul>
                <li>I believe that the design differences between Versions A and B will influence the number of misclicks on each version, thus leading to 
                  different distributions of misclicks for each version. Version A has twice as many buttons as Version B, meaning there are more options for users 
                  to click. Unlike Version A, Version B’s clearer color contrast between the button color/text and the hover effect of outlining a hovered button 
                  in black makes the text on buttons more legible and provides the user with visual confirmation that the button is “clickable.” Additionally, the 
                  dividers in Version B provide more structure and assist users in identifying which button corresponds with which appointment. Lastly, appointments 
                  in Version B are ordered chronologically. These design features in Version B will assist users in determining whether the button they want to 
                  click is the correct one or not. Hence, I believe that the distribution of misclicks will be different for Version A and Version B.
                </li>
              </ul>
          </ul>
            
        <p class="bodyText additionalIndent">Statistical Testing</p>
          <ul class="additionalIndent">
            <li><span class="boldText">Type Of Test:</span> The metric I am using is did_misclick which is a categorical value that can either be true (did misclick) 
              or false (didn't misclick). Since the metric is categorical, I chose to use a chi-squared test. 
            </li>
            <li><span class="boldText">Statistical Significance & Important Values:</span> I found a statistically significant difference in the distribution of 
              misclicks for Versions A and B, as p<=0.05. The p-value = 0.0399, meaning there is only a 3.99% chance that Version A and Version B have the same 
              misclick distribution. The chi^squared statistic records the magnitude of difference between the misclick distributions for Versions A and B. A 
              chi^squared statistic that is greater than 2.5 is extreme. Here, the chi^squared statistic = 4.2207, indicating that there is a large magnitude of 
              difference between Version A and Version B’s misclick distributions. The df (degrees of freedom) = 1, which correctly indicates that there is only 
              one category being compared: did_misclick. A theoretical distribution where the misclick distributions for Versions A and B are the same expects 5 
              users to misclick and 29 users not to misclick (17.24% of users to misclick). However, the observed frequency of misclicking deviates from the 
              expected value, as 24% of users misclicked on Version A and 6% of users misclicked on Version B.
            </li>
            <li><span class="boldText">Conclusion:</span> I reject the null hypothesis and find statistically significant evidence that the alternative hypothesis 
              is true, since p-value <=0.05. There is only a 3.99% chance that Versions A and B will have the same misclick distribution, and therefore only a 
              3.99% chance that the null hypothesis is true. Additionally, a theoretical distribution where Versions A and B have the same misclick distribution 
              expects 17.24% of users to misclick. However, 24% of users misclicked on Version A and 6% of users misclicked on Version B. These deviations from 
              the expected misclick distribution highlight the differences between Version A and Version B’s misclick distributions. Thus, I reject the null 
              hypothesis and am confident that the alternative hypothesis is true.
            </li>
          </ul>
        
          <!-- Time On Page -->
        <h4 class="subHeaderText">ii. Time On Page</h4>
        <p class="bodyText additionalIndent">Hypotheses</p>
          <ul class="additionalIndent">
            <li><span class="boldText">Metric Of Choice:</span> The metric of choice is time_on_page, which is the total time that the user spent on the page in 
              milliseconds.</li>
            <li><span class="boldText">Null Hypothesis:</span> The amount of time spent Version A is the same as the amount of time spent on Version B.</li>
              <ul>
                <li>I predict that I will reject the null hypothesis. As noted in my justification for my alternative hypothesis, design changes introduced in 
                  Version B include fewer buttons, clearer color contrast for the buttons, button hover effects, adding dividers, and ordering the appointments 
                  chronologically. These design modifications were strategically made to improve the readability and structure of the webpage. I predict the design 
                  differences between Versions A and B will influence user interaction with the page. After skimming the data, it appears that more users spent a 
                  longer time on Version A. Due to the design differences between Versions A and B and the observed discrepancy between the two versions’ 
                  time_on_page metric, I predict that the statistical test’s results will find statistically significant differences in the amount of time spent on 
                  Versions A and B. Thus, I predict that I will find statistically significant differences and reject the null hypothesis.
                </li>
              </ul>
            <li><span class="boldText">Alternative Hypothesis:</span> The amount of time spent on Version A is greater than the amount of time spent on Version B.</li>
              <ul>
                <li>I believe that the design differences between Versions A and B will influence the amount of time spent on each version, resulting in users 
                  spending more time on Version A than on Version B. Version A has twice as many buttons as Version B, meaning there are more options that users 
                  must read over and process. Unlike Version A, Version B’s clearer color contrast between the button color/text and the hover effect of outlining 
                  a hovered button in black makes the text on buttons more legible and provides the user with visual confirmation that they are hovering the correct 
                  button. Additionally, the dividers in Version B provide more structure for users as they visually separate which button corresponds with which 
                  appointment. Lastly, appointments in Version B are ordered chronologically, which is a standardized ordering method that users are accustomed to 
                  seeing. These design features in Version B will assist users in navigating the webpage by providing more readability and structure, thus 
                  decreasing the time necessary to complete various tasks. Hence, I believe that the amount of time spent on Version A is greater than the amount 
                  of time spent on Version B.
                </li>
              </ul>
          </ul>
        <p class="bodyText additionalIndent">Statistical Testing</p>
          <ul class="additionalIndent">
            <li><span class="boldText">Type Of Test:</span> The metric I am using is time_on_page which is a continuous value. Since the metric is 
              continuous, I chose to use a t-test. The alternative hypothesis states “greater than,” so I will use a one-tailed t-test (rather than a two-tailed 
              t-test). 
            </li>
            <li><span class="boldText">Statistical Significance & Important Values:</span> xyz </li>
            <li><span class="boldText">Conclusion:</span> xyz </li>
          </ul>

        <!-- Time Until User Executed First Click -->
        <h4 class="subHeaderText">iii. Time Until User Executed First Click</h4>
        <p class="bodyText additionalIndent">Hypotheses</p>
          <ul class="additionalIndent">
            <li><span class="boldText">Metric Of Choice:</span> The metric of choice for success rate is time_to_first_click, which is the time it took for the user 
              to execute his/her first click in milliseconds.</li>
            <li><span class="boldText">Null Hypothesis:</span> The time until the user first clicked on Version A is the same as the time until the user first 
              clicked on Version B.</li>
              <ul>
                <li>I predict that I will reject the null hypothesis. As noted in my justification for my alternative hypothesis, design changes introduced in 
                  Version B include fewer buttons, clearer color contrast for the buttons, button hover effects, adding dividers, and ordering the appointments 
                  chronologically. These design modifications were strategically made to improve the readability and structure of the webpage. I predict the design 
                  differences between Versions A and B will influence user interaction with the page. After skimming the data, it appears that more users spent a 
                  longer time before clicking for the first time on Version A. Due to the design differences between Versions A and B and the observed discrepancy 
                  between the two versions’ time_to_first_click metric, I predict that the statistical test’s results will find statistically significant 
                  differences in the time spent before the user first clicked on Versions A and B. Thus, I predict that I will find statistically significant 
                  differences and reject the null hypothesis.
                </li>
              </ul>
            <li><span class="boldText">Alternative Hypothesis:</span> The time until the user first clicked on Version A is greater than the time until the user 
              first clicked on Version B.</li>
              <ul>
                <li>I believe that the design differences between Versions A and B will influence the time spent before the user first clicked on each version, 
                  resulting in users taking longer to first click on Version A than on Version B. Version A has twice as many buttons as Version B, meaning there 
                  are more options that users must read over and process. Unlike Version A, Version B’s clearer color contrast between the button color/text and 
                  the hover effect of outlining a hovered button in black makes the text on buttons more legible and provides the user with visual confirmation 
                  that they are hovering the correct button. Additionally, the dividers in Version B provide more structure for users as they visually separate
                  which button corresponds with which appointment. Lastly, appointments in Version B are ordered chronologically, which is a standardized ordering 
                  method that users are accustomed to seeing. These design features in Version B will assist users in navigating the webpage by providing more 
                  readability and structure, thus decreasing the time needed to execute the first click. Hence, I believe that the time spent before the user first 
                  clicked on Version A is greater than the time spent before the user first clicked on Version B.
                </li>
              </ul>
          </ul>
        <p class="bodyText additionalIndent">Statistical Testing</p>
          <ul class="additionalIndent">
            <li><span class="boldText">Type Of Test:</span> The metric I am using is time_to_first_click which is a continuous value. Since the metric 
              is continuous, I chose to use a t-test. The alternative hypothesis states “greater than,” so I will use a one-tailed t-test (rather than a two-tailed 
              t-test). 
            </li>
            <li><span class="boldText">Statistical Significance & Important Values:</span> I found a statistically significant difference in the amount of time 
              before the user executed the first click for Versions A and B, as p<=0.05. The p-value = 0.005, meaning there is only a 0.5% chance that the time 
              until the user first clicked for Version A and Version B was the same. The t-score records the magnitude of difference between the amount of time 
              spent on Versions A and B. A t-score that is greater than 2.5 is considered extreme. Here, the t-score = 2.6573, indicating that there is a large 
              magnitude of difference between the time until the user first clicked on Versions A and B. The average amount of time until the user first clicked 
              for Version A was 5542 milliseconds, or 5.542 seconds. The average amount of time until the user first clicked for Version B was 3427 milliseconds, 
              or 3.427 seconds. The average time before the user first clicked on Version B is 2.115 seconds faster than the average time before the user first 
              clicked on Version A. The variance for Version A was 19,411,964 milliseconds-squared, or 4.41 seconds, and the variance for Version B was 2,201,393 
              milliseconds-squared, or 1.48 seconds. There was more variability in the time spent before the user first clicked for Version A, meaning that the 
              spread of the data in relation to the mean was larger for Version A. Lastly, Welch-Satterthwaite equation was used to approximate the degrees of 
              freedom, which represent the number of independent pieces of data being considered. This equation accounts for variability in the data. The df = 40, 
              which is relatively close to the actual number of users, which was 34. The degrees of freedom exceeded the number of users, suggesting that there 
              might be large outliers in the data. 
            </li>
            <li><span class="boldText">Conclusion:</span> I reject the null hypothesis and find statistically significant evidence that the alternative hypothesis 
              is true, since p-value <= 0.05. There is only a 0.5% chance that the time until the user first clicked was the same for Versions A and B, and 
              therefore only a 0.5% change that the null hypothesis is true. Additionally, on average, the amount of time before users first clicked was 2.115 
              more seconds on Version A. The variance for Version A was 4.41 seconds, whereas the variance for Version B was 1.48 seconds. Version B has a lower 
              variance than Version A. This difference suggests that users were able to complete the assigned task more consistently in Version B, whereas users 
              likely had more diverse experiences when interacting with Version A. Thus, I reject the null hypothesis and am confident that the alternative 
              hypothesis is true.
            </li>
          </ul>
        
        <h4 class="subHeaderText">Summary</h4>
      </div>

      <!-- Reflection-->
      <div class="sectionSpecifications">
        <h3 class="headerText">IV. Reflection</h3>
        <p class="bodyText">I learned a lot from the process of gathering data and conducting an A/B test. I have a much better understanding of how to interpret 
          the results of statistical tests now, and I was surprised to see how impactful small design changes can be on users’ interactions with webpages. It’s 
          also interesting to think about which metrics are the most important – for example, knowing that users spend more time on Version A than Version B 
          doesn’t necessarily make Version A a worse design choice. It’s especially interesting to consider the variability of the data and how consistently users 
          interact with the webpage. Outliers can significantly increase the variability, and it’s hard to determine whether those few outliers represent a 
          significant amount of the user base (enough to try and fix the issues they encounter) or not. 
        </p>
        <p class="bodyText">I would be interested in conducting an A/B test outside of our studio environment, since I feel as though the data collection method 
          in studio significantly influenced results. With this said, I enjoyed this project and definitely refined my statistical analysis skills!
        </p>
      </div>

    </body>
